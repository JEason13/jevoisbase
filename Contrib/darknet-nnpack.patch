diff --git a/src/network.c b/src/network.c
index aaab799..90d713d 100644
--- a/src/network.c
+++ b/src/network.c
@@ -195,6 +195,9 @@ void forward_network(network *netp)
 #endif
     network net = *netp;
     int i;
+    char tmp[256]; // JEVOIS
+    
+    jevois_darknet_profiler_start(); // JEVOIS
     for(i = 0; i < net.n; ++i){
         net.index = i;
         layer l = net.layers[i];
@@ -206,8 +209,11 @@ void forward_network(network *netp)
         if(l.truth) {
             net.truth = l.output;
         }
+	snprintf(tmp, 255, "%3d - %s", i, get_layer_string(l.type)); // JEVOIS
+	jevois_darknet_profiler_checkpoint(tmp); // JEVOIS
     }
     calc_network_cost(netp);
+    jevois_darknet_profiler_stop(); // JEVOIS
 }
 
 void update_network(network *netp)
@@ -769,6 +775,9 @@ void forward_network_gpu(network *netp)
     }
 
     int i;
+    char tmp[256]; // JEVOIS
+    
+    jevois_darknet_profiler_start(); // JEVOIS
     for(i = 0; i < net.n; ++i){
         net.index = i;
         layer l = net.layers[i];
diff --git a/src/parser.c b/src/parser.c
index 8b43f3c..8f4fdb2 100644
--- a/src/parser.c
+++ b/src/parser.c
@@ -43,6 +43,9 @@ typedef struct{
 
 list *read_cfg(char *filename);
 
+#define JVBEG fprintf(stderr, "\n##### %d:%s() - BEG #####\n", __LINE__, __PRETTY_FUNCTION__)
+#define JVEND fprintf(stderr, "\n@@@@@ %d:%s() - END @@@@@\n", __LINE__, __PRETTY_FUNCTION__)
+
 LAYER_TYPE string_to_layer_type(char * type)
 {
 
@@ -128,8 +131,8 @@ typedef struct size_params{
 } size_params;
 
 local_layer parse_local(list *options, size_params params)
-{
-    int n = option_find_int(options, "filters",1);
+{ JVBEG;
+  int n = option_find_int(options, "filters",1);
     int size = option_find_int(options, "size",1);
     int stride = option_find_int(options, "stride",1);
     int pad = option_find_int(options, "pad",0);
@@ -145,11 +148,11 @@ local_layer parse_local(list *options, size_params params)
 
     local_layer layer = make_local_layer(batch,h,w,c,n,size,stride,pad,activation);
 
-    return layer;
+    JVEND; return layer;
 }
 
 layer parse_deconvolutional(list *options, size_params params)
-{
+{ JVBEG;
     int n = option_find_int(options, "filters",1);
     int size = option_find_int(options, "size",1);
     int stride = option_find_int(options, "stride",1);
@@ -170,12 +173,12 @@ layer parse_deconvolutional(list *options, size_params params)
 
     layer l = make_deconvolutional_layer(batch,h,w,c,n,size,stride,padding, activation, batch_normalize, params.net->adam);
 
-    return l;
+    JVEND; return l;
 }
 
 
 convolutional_layer parse_convolutional(list *options, size_params params)
-{
+{ JVBEG;
     int n = option_find_int(options, "filters",1);
     int size = option_find_int(options, "size",1);
     int stride = option_find_int(options, "stride",1);
@@ -201,11 +204,11 @@ convolutional_layer parse_convolutional(list *options, size_params params)
     layer.flipped = option_find_int_quiet(options, "flipped", 0);
     layer.dot = option_find_float_quiet(options, "dot", 0);
 
-    return layer;
+    JVEND; return layer;
 }
 
 layer parse_crnn(list *options, size_params params)
-{
+{ JVBEG;
     int output_filters = option_find_int(options, "output_filters",1);
     int hidden_filters = option_find_int(options, "hidden_filters",1);
     char *activation_s = option_find_str(options, "activation", "logistic");
@@ -216,11 +219,11 @@ layer parse_crnn(list *options, size_params params)
 
     l.shortcut = option_find_int_quiet(options, "shortcut", 0);
 
-    return l;
+    JVEND; return l;
 }
 
 layer parse_rnn(list *options, size_params params)
-{
+{ JVBEG;
     int output = option_find_int(options, "output",1);
     char *activation_s = option_find_str(options, "activation", "logistic");
     ACTIVATION activation = get_activation(activation_s);
@@ -230,43 +233,43 @@ layer parse_rnn(list *options, size_params params)
 
     l.shortcut = option_find_int_quiet(options, "shortcut", 0);
 
-    return l;
+    JVEND; return l;
 }
 
 layer parse_gru(list *options, size_params params)
-{
+{ JVBEG;
     int output = option_find_int(options, "output",1);
     int batch_normalize = option_find_int_quiet(options, "batch_normalize", 0);
 
     layer l = make_gru_layer(params.batch, params.inputs, output, params.time_steps, batch_normalize, params.net->adam);
     l.tanh = option_find_int_quiet(options, "tanh", 0);
 
-    return l;
+    JVEND; return l;
 }
 
 layer parse_lstm(list *options, size_params params)
-{
+{ JVBEG;
     int output = option_find_int(options, "output", 1);
     int batch_normalize = option_find_int_quiet(options, "batch_normalize", 0);
 
     layer l = make_lstm_layer(params.batch, params.inputs, output, params.time_steps, batch_normalize, params.net->adam);
 
-    return l;
+    JVEND; return l;
 }
 
 layer parse_connected(list *options, size_params params)
-{
+{ JVBEG;
     int output = option_find_int(options, "output",1);
     char *activation_s = option_find_str(options, "activation", "logistic");
     ACTIVATION activation = get_activation(activation_s);
     int batch_normalize = option_find_int_quiet(options, "batch_normalize", 0);
 
     layer l = make_connected_layer(params.batch, params.inputs, output, activation, batch_normalize, params.net->adam);
-    return l;
+    JVEND; return l;
 }
 
 softmax_layer parse_softmax(list *options, size_params params)
-{
+{ JVBEG;
     int groups = option_find_int_quiet(options, "groups",1);
     softmax_layer layer = make_softmax_layer(params.batch, params.inputs, groups);
     layer.temperature = option_find_float_quiet(options, "temperature", 1);
@@ -276,11 +279,11 @@ softmax_layer parse_softmax(list *options, size_params params)
     layer.h = params.h;
     layer.c = params.c;
     layer.spatial = option_find_float_quiet(options, "spatial", 0);
-    return layer;
+    JVEND; return layer;
 }
 
 int *parse_yolo_mask(char *a, int *num)
-{
+{ JVBEG;
     int *mask = 0;
     if(a){
         int len = strlen(a);
@@ -297,11 +300,11 @@ int *parse_yolo_mask(char *a, int *num)
         }
         *num = n;
     }
-    return mask;
+    JVEND; return mask;
 }
 
 layer parse_yolo(list *options, size_params params)
-{
+{ JVBEG;
     int classes = option_find_int(options, "classes", 20);
     int total = option_find_int(options, "num", 1);
     int num = total;
@@ -335,11 +338,11 @@ layer parse_yolo(list *options, size_params params)
             a = strchr(a, ',')+1;
         }
     }
-    return l;
+    JVEND; return l;
 }
 
 layer parse_region(list *options, size_params params)
-{
+{ JVBEG;
     int coords = option_find_int(options, "coords", 4);
     int classes = option_find_int(options, "classes", 20);
     int num = option_find_int(options, "num", 1);
@@ -387,11 +390,11 @@ layer parse_region(list *options, size_params params)
             a = strchr(a, ',')+1;
         }
     }
-    return l;
+    JVEND; return l;
 }
 
 detection_layer parse_detection(list *options, size_params params)
-{
+{ JVBEG;
     int coords = option_find_int(options, "coords", 1);
     int classes = option_find_int(options, "classes", 1);
     int rescore = option_find_int(options, "rescore", 0);
@@ -411,11 +414,11 @@ detection_layer parse_detection(list *options, size_params params)
     layer.jitter = option_find_float(options, "jitter", .2);
     layer.random = option_find_int_quiet(options, "random", 0);
     layer.reorg = option_find_int_quiet(options, "reorg", 0);
-    return layer;
+    JVEND; return layer;
 }
 
 cost_layer parse_cost(list *options, size_params params)
-{
+{ JVBEG;
     char *type_s = option_find_str(options, "type", "sse");
     COST_TYPE type = get_cost_type(type_s);
     float scale = option_find_float_quiet(options, "scale",1);
@@ -423,11 +426,11 @@ cost_layer parse_cost(list *options, size_params params)
     layer.ratio =  option_find_float_quiet(options, "ratio",0);
     layer.noobject_scale =  option_find_float_quiet(options, "noobj", 1);
     layer.thresh =  option_find_float_quiet(options, "thresh",0);
-    return layer;
+    JVEND; return layer;
 }
 
 crop_layer parse_crop(list *options, size_params params)
-{
+{ JVBEG;
     int crop_height = option_find_int(options, "crop_height",1);
     int crop_width = option_find_int(options, "crop_width",1);
     int flip = option_find_int(options, "flip",0);
@@ -447,11 +450,11 @@ crop_layer parse_crop(list *options, size_params params)
     crop_layer l = make_crop_layer(batch,h,w,c,crop_height,crop_width,flip, angle, saturation, exposure);
     l.shift = option_find_float(options, "shift", 0);
     l.noadjust = noadjust;
-    return l;
+    JVEND; return l;
 }
 
 layer parse_reorg(list *options, size_params params)
-{
+{ JVBEG;
     int stride = option_find_int(options, "stride",1);
     int reverse = option_find_int_quiet(options, "reverse",0);
     int flatten = option_find_int_quiet(options, "flatten",0);
@@ -465,11 +468,11 @@ layer parse_reorg(list *options, size_params params)
     if(!(h && w && c)) error("Layer before reorg layer must output image.");
 
     layer layer = make_reorg_layer(batch,w,h,c,stride,reverse, flatten, extra);
-    return layer;
+    JVEND; return layer;
 }
 
 maxpool_layer parse_maxpool(list *options, size_params params)
-{
+{ JVBEG;
     int stride = option_find_int(options, "stride",1);
     int size = option_find_int(options, "size",stride);
     int padding = option_find_int_quiet(options, "padding", (size-1)/2);
@@ -482,11 +485,11 @@ maxpool_layer parse_maxpool(list *options, size_params params)
     if(!(h && w && c)) error("Layer before maxpool layer must output image.");
 
     maxpool_layer layer = make_maxpool_layer(batch,h,w,c,size,stride,padding);
-    return layer;
+    JVEND; return layer;
 }
 
 avgpool_layer parse_avgpool(list *options, size_params params)
-{
+{ JVBEG;
     int batch,w,h,c;
     w = params.w;
     h = params.h;
@@ -495,37 +498,37 @@ avgpool_layer parse_avgpool(list *options, size_params params)
     if(!(h && w && c)) error("Layer before avgpool layer must output image.");
 
     avgpool_layer layer = make_avgpool_layer(batch,w,h,c);
-    return layer;
+    JVEND; return layer;
 }
 
 dropout_layer parse_dropout(list *options, size_params params)
-{
+{ JVBEG;
     float probability = option_find_float(options, "probability", .5);
     dropout_layer layer = make_dropout_layer(params.batch, params.inputs, probability);
     layer.out_w = params.w;
     layer.out_h = params.h;
     layer.out_c = params.c;
-    return layer;
+    JVEND; return layer;
 }
 
 layer parse_normalization(list *options, size_params params)
-{
+{ JVBEG;
     float alpha = option_find_float(options, "alpha", .0001);
     float beta =  option_find_float(options, "beta" , .75);
     float kappa = option_find_float(options, "kappa", 1);
     int size = option_find_int(options, "size", 5);
     layer l = make_normalization_layer(params.batch, params.w, params.h, params.c, size, alpha, beta, kappa);
-    return l;
+    JVEND; return l;
 }
 
 layer parse_batchnorm(list *options, size_params params)
-{
+{ JVBEG;
     layer l = make_batchnorm_layer(params.batch, params.w, params.h, params.c);
-    return l;
+    JVEND; return l;
 }
 
 layer parse_shortcut(list *options, size_params params, network *net)
-{
+{ JVBEG;
     char *l = option_find(options, "from");
     int index = atoi(l);
     if(index < 0) index = params.index + index;
@@ -540,31 +543,31 @@ layer parse_shortcut(list *options, size_params params, network *net)
     s.activation = activation;
     s.alpha = option_find_float_quiet(options, "alpha", 1);
     s.beta = option_find_float_quiet(options, "beta", 1);
-    return s;
+    JVEND; return s;
 }
 
 
 layer parse_l2norm(list *options, size_params params)
-{
+{ JVBEG;
     layer l = make_l2norm_layer(params.batch, params.inputs);
     l.h = l.out_h = params.h;
     l.w = l.out_w = params.w;
     l.c = l.out_c = params.c;
-    return l;
+    JVEND; return l;
 }
 
 
 layer parse_logistic(list *options, size_params params)
-{
+{ JVBEG;
     layer l = make_logistic_layer(params.batch, params.inputs);
     l.h = l.out_h = params.h;
     l.w = l.out_w = params.w;
     l.c = l.out_c = params.c;
-    return l;
+    JVEND; return l;
 }
 
 layer parse_activation(list *options, size_params params)
-{
+{ JVBEG;
     char *activation_s = option_find_str(options, "activation", "linear");
     ACTIVATION activation = get_activation(activation_s);
 
@@ -574,20 +577,20 @@ layer parse_activation(list *options, size_params params)
     l.w = l.out_w = params.w;
     l.c = l.out_c = params.c;
 
-    return l;
+    JVEND; return l;
 }
 
 layer parse_upsample(list *options, size_params params, network *net)
-{
+{ JVBEG;
 
     int stride = option_find_int(options, "stride",2);
     layer l = make_upsample_layer(params.batch, params.w, params.h, params.c, stride);
     l.scale = option_find_float_quiet(options, "scale", 1);
-    return l;
+    JVEND; return l;
 }
 
 route_layer parse_route(list *options, size_params params, network *net)
-{
+{ JVBEG;
     char *l = option_find(options, "layers");
     int len = strlen(l);
     if(!l) error("Route Layer must specify input layers");
@@ -624,11 +627,11 @@ route_layer parse_route(list *options, size_params params, network *net)
         }
     }
 
-    return layer;
+    JVEND; return layer;
 }
 
 learning_rate_policy get_policy(char *s)
-{
+{ JVBEG;
     if (strcmp(s, "random")==0) return RANDOM;
     if (strcmp(s, "poly")==0) return POLY;
     if (strcmp(s, "constant")==0) return CONSTANT;
@@ -637,11 +640,11 @@ learning_rate_policy get_policy(char *s)
     if (strcmp(s, "sigmoid")==0) return SIG;
     if (strcmp(s, "steps")==0) return STEPS;
     fprintf(stderr, "Couldn't find policy %s, going with constant\n", s);
-    return CONSTANT;
+    JVEND; return CONSTANT;
 }
 
 void parse_net_options(list *options, network *net)
-{
+{ JVBEG;
     net->batch = option_find_int(options, "batch",1);
     net->learning_rate = option_find_float(options, "learning_rate", .001);
     net->momentum = option_find_float(options, "momentum", .9);
@@ -728,7 +731,7 @@ int is_network(section *s)
 }
 
 network *parse_network_cfg(char *filename)
-{
+{ JVBEG;
     list *sections = read_cfg(filename);
     node *n = sections->front;
     if(!n) error("Config file has no sections");
@@ -754,6 +757,7 @@ network *parse_network_cfg(char *filename)
     int count = 0;
     free_section(s);
     fprintf(stderr, "layer     filters    size              input                output\n");
+
     while(n){
         params.index = count;
         fprintf(stderr, "%5d ", count);
@@ -870,7 +874,7 @@ network *parse_network_cfg(char *filename)
         net->workspace = calloc(1, workspace_size);
 #endif
     }
-    return net;
+    JVEND; return net;
 }
 
 list *read_cfg(char *filename)
@@ -1218,14 +1222,15 @@ void load_weights_upto(network *net, char *filename, int start, int cutoff)
     fread(&minor, sizeof(int), 1, fp);
     fread(&revision, sizeof(int), 1, fp);
     if ((major*10 + minor) >= 2 && major < 1000 && minor < 1000){
-        fread(net->seen, sizeof(size_t), 1, fp);
+      unsigned long long storing_size_t_is_not_portable;
+      fread(&storing_size_t_is_not_portable, sizeof(unsigned long long), 1, fp);
+      *net->seen = storing_size_t_is_not_portable;
     } else {
         int iseen = 0;
         fread(&iseen, sizeof(int), 1, fp);
         *net->seen = iseen;
     }
     int transpose = (major > 1000) || (minor > 1000);
-
     int i;
     for(i = start; i < net->n && i < cutoff; ++i){
         layer l = net->layers[i];
